---
phase: 02-data-pipeline
plan: 02-02
title: File Import Backend
wave: 1
status: pending
requirements: [IMPORT-01, IMPORT-02, IMPORT-03, IMPORT-04]
depends_on: []
created: 2026-01-16
---

## Goal

Build the backend file parsing infrastructure for HPLC export files. Support TXT tab-delimited format initially. Create jobs and samples from imported files.

## Context

HPLC instruments export results as tab-delimited TXT files. Each file contains header rows followed by data rows with columns like retention time, peak area, peak height, etc. The parser needs to handle variable column positions using the column mappings from settings.

## Tasks

### Task 1: File Parser Module

**Files to create:**
- `backend/parsers/__init__.py` - Parser module init
- `backend/parsers/txt_parser.py` - TXT file parser

**Implementation:**

1. Create parsers package structure

2. TXT parser implementation:
```python
def parse_txt_file(file_path: Path, column_mappings: dict) -> ParseResult:
    """
    Parse tab-delimited TXT file from HPLC export.

    Args:
        file_path: Path to TXT file
        column_mappings: Dict mapping semantic names to column headers

    Returns:
        ParseResult with rows of extracted data
    """
```

3. ParseResult dataclass:
```python
@dataclass
class ParseResult:
    filename: str
    rows: list[dict]  # Each row as dict with mapped column names
    raw_headers: list[str]
    row_count: int
    errors: list[str]
```

4. Handle common variations:
   - Skip header rows until column headers found
   - Map column headers to semantic names
   - Convert numeric values (handle commas, scientific notation)

### Task 2: Import API Endpoints

**Files to modify:**
- `backend/main.py` - Add import endpoints

**Implementation:**

1. Add import endpoints:
   - `POST /import/file` - Import single file, return parsed preview
   - `POST /import/batch` - Import multiple files, create job

2. Import file endpoint:
```python
@app.post("/import/file")
async def import_file(file_path: str, db: Session = Depends(get_db)):
    # Get column mappings from settings
    # Parse file
    # Return preview (first 10 rows) without saving
```

3. Batch import endpoint:
```python
@app.post("/import/batch")
async def import_batch(file_paths: list[str], db: Session = Depends(get_db)):
    # Create Job record
    # For each file:
    #   - Parse file
    #   - Create Sample record
    #   - Store parsed data in Sample or separate table
    # Return job_id and summary
```

### Task 3: Import TypeScript Client

**Files to modify:**
- `src/lib/api.ts` - Add import API functions

**Implementation:**

1. Add typed interfaces:
```typescript
interface ParsePreview {
  filename: string
  headers: string[]
  rows: Record<string, string | number>[]
  row_count: number
  errors: string[]
}

interface ImportResult {
  job_id: number
  samples_created: number
  errors: string[]
}
```

2. Add API functions:
   - `previewFile(filePath: string): Promise<ParsePreview>`
   - `importBatch(filePaths: string[]): Promise<ImportResult>`

## Verification

```bash
# Create test TXT file
echo -e "Sample\tRT\tArea\tHeight\nTest1\t1.234\t56789\t1234" > test_hplc.txt

# Test file preview
curl -X POST "http://127.0.0.1:8009/import/file?file_path=test_hplc.txt"

# Test batch import
curl -X POST http://127.0.0.1:8009/import/batch \
  -H "Content-Type: application/json" \
  -d '{"file_paths": ["test_hplc.txt"]}'

# Verify job and sample created
curl http://127.0.0.1:8009/jobs
curl http://127.0.0.1:8009/samples
```

## Must-Haves

- [ ] TXT parser extracts data rows correctly
- [ ] Column mapping applies to extracted data
- [ ] Preview endpoint returns parsed data without saving
- [ ] Batch import creates Job and Sample records
- [ ] Parse errors captured and returned (not silent failures)

## Notes

- Start with TXT only; CSV/Excel parsers can be added later
- Store raw parsed data in Sample.input_data JSON field
- File paths come from frontend; backend doesn't browse filesystem
- Audit log entries for all imports
